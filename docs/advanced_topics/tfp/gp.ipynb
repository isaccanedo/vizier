{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOp-9XGzKYIx"
      },
      "source": [
        "# Bayesian Optimization Modeling\n",
        "The goal of this tutorial is to introduce Bayesian optimization workflows in OSS Vizier, including the underlying TensorFlow Probability (TFP) components and JAX/Flax functionality. The target audience is researchers and practitioners already well-versed in Bayesian optimization, who want to **define and train their own Gaussian Process surrogate models** for Bayesian optimization in OSS Vizier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WzyCnk228YLU"
      },
      "source": [
        "## Additional resources for TFP\n",
        "If you're new to TFP, a good place to start is [A tour of TensorFlow Probability](https://www.tensorflow.org/probability/examples/A_Tour_of_TensorFlow_Probability). TFP began as a TensorFlow-only library, but now has a [JAX backend](https://www.tensorflow.org/probability/examples/TensorFlow_Probability_on_JAX) that is entirely independent of TensorFlow (such that \"Tensor-Friendly Probability\" might be a better backronym). This Colab uses TFP's JAX backend (see the \"Imports\" cell for how to import it)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdxDxRtW8YtL"
      },
      "source": [
        "## Additional resources for Flax\n",
        "OSS Vizier's Bayesian Optimization models are defined as [Flax](https://flax.readthedocs.io/en/latest/) modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH8pFLCoOjZl"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrW3fCmYv9MC"
      },
      "outputs": [],
      "source": [
        "import chex\n",
        "import jax\n",
        "from jax import numpy as jnp, random, tree_util\n",
        "import numpy as np\n",
        "import optax\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow_probability.substrates import jax as tfp\n",
        "from typing import Any\n",
        "\n",
        "# Vizier models can freely access modules from vizier._src\n",
        "from vizier._src.benchmarks.experimenters.synthetic import bbob\n",
        "from vizier._src.jax.optimizers import optimizers\n",
        "from vizier._src.jax import stochastic_process_model as spm\n",
        "\n",
        "tfd = tfp.distributions\n",
        "tfb = tfp.bijectors\n",
        "tfpk = tfp.math.psd_kernels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dw_N-81ANozw"
      },
      "source": [
        "## Defining a GP surrogate model and hyperparameters\n",
        "\n",
        "To write a GP surrogate model, first write a coroutine that yields parameter specifications (`ModelParameter`) and returns a GP distribution. Downstream, the parameter specifications are used to define Flax module parameters. The inputs to the coroutine function represent the index points of the GP (in the remainder of this Colab, we refer to \"inputs\" and \"index points\" interchangeably).\n",
        "\n",
        "The rationale for the coroutine design is that it lets us automate the application of the parameter constraint and initialization functions (corresponding to hyperpriors, e.g.), and enables simultaneous specification of the model parameters and how their values are used to instantiate a GP.\n",
        "\n",
        "### Coroutine example\n",
        "\n",
        "The following cell shows a coroutine defining a GP with a squared exponential kernel and two parameters: the length scale of the kernel and the observation noise variance of the GP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMBRd_msy5Iq"
      },
      "outputs": [],
      "source": [
        "def simple_gp_coroutine(inputs: chex.Array=None):\n",
        "  length_scale = yield spm.ModelParameter.from_prior(\n",
        "      tfd.Gamma(1., 1., name='length_scale'))\n",
        "  amplitude = 2.  # Non-trainable parameters may be defined as constants.\n",
        "  kernel = tfpk.ExponentiatedQuadratic(\n",
        "      amplitude=amplitude, length_scale=length_scale)\n",
        "  observation_noise_variance = yield spm.ModelParameter(\n",
        "      init_fn=lambda x: jnp.exp(random.normal(x)),\n",
        "      constraint=spm.Constraint(bounds=(0.0, 100.0), bijector=tfb.Softplus()),\n",
        "      regularizer=lambda x: x**2,\n",
        "      name='observation_noise_variance')\n",
        "  return tfd.GaussianProcess(\n",
        "      kernel,\n",
        "      index_points=inputs,\n",
        "      observation_noise_variance=observation_noise_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXWmOlqUy9Jc"
      },
      "source": [
        "## ModelParameter\n",
        "\n",
        "`ModelParameter` may be used to define hyperpriors.\n",
        "\n",
        "### Parameter specifications from priors\n",
        "\n",
        "The length scale parameter has a Gamma prior. This is equivalent to defining a `ModelParameter` with a regularizer that computes the Gamma negative log likelihood and an initialization function that samples from the Gamma distribution. As the constraint was not specified, a default one is assigned which is the \"default event space bijector\" of the TFP distribution (each TFP distribution has a constraining bijector that maps the real line to the support of the distribution).\n",
        "\n",
        "### Specifying parameters explicitly\n",
        "\n",
        "Observation noise variance, which is passed to the Gaussian process and represents the scalar variance of zero-mean Gaussian noise in the observed labels, is not given a `tfd.Distribution` prior. Instead, it has its initialization, constraining, and regularization functions defined individually. Note that the initialization function is in the constrained space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhXNY46XL9SJ"
      },
      "source": [
        "## Constraints\n",
        "\n",
        "ModelParameter allows to define constraints on the model parameters using the 'Constraint' object which is initiated with a tuple of 'bounds' and 'bijector' function.\n",
        "\n",
        "Though the constraints are defined as part of the ModelParameter the Flax model itself does not use them, but rather it expects to receive parameter values already in the constrained space. This means that it's the responsibility of the user/optimizer to pass the GP parameter values that are already in the constrained space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0l40fQxzOMS"
      },
      "source": [
        "### Exercise: Write a GP model\n",
        "Write an ARD Gaussian Process model with three parameters: `signal_variance`, `length_scale`, and `observation_noise_variance`. (This is a slightly simplified version of the Vizier GP.)\n",
        "- `signal_variance` and `observation noise_variance` are both:\n",
        "  - regularized by the function $f(x) = 0.01\\log(x)^2$\n",
        "  - bounded to be positive.\n",
        "- `signal_variance` parameterizes a Matern 5/2 kernel, where the amplitude of the kernel is the square root of `signal_variance`. Use [`tfpk.MaternFiveHalves`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/math/psd_kernels/matern.py#L414).\n",
        "- `length_scale` has a $LogNormal(0, 1)$ prior for each dimension. Assume there are 4 dimensions, and use [`tfd.Sample`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/distributions/sample.py) to build a 4-dimensional distribution consisting of IID LogNormal distributions. (Note that the `length_scale` parameter is a vector -- all other parameters are scalars.)\n",
        "- In TFP, ARD kernels are implemented with [`tfpk.FeatureScaled`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/math/psd_kernels/feature_scaled.py), with `scale_diag` representing the length scale along each dimension.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XluuKkwFD4ns"
      },
      "outputs": [],
      "source": [
        "def vizier_gp_coroutine(inputs: chex.Array=None):\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyWiHvaaMKTz"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aj7pJgA8zU4y"
      },
      "outputs": [],
      "source": [
        "data_dimensionality = 2\n",
        "\n",
        "def vizier_gp_coroutine(inputs: chex.Array=None):\n",
        "  \"\"\"A coroutine that follows the `ModelCoroutine` protocol.\"\"\"\n",
        "  signal_variance = yield spm.ModelParameter(\n",
        "      init_fn=lambda x: tfb.Softplus()(random.normal(x)),\n",
        "      constraint=spm.Constraint(bounds=(0.0, 100.0), bijector=tfb.Softplus()),\n",
        "      regularizer=lambda x: 0.01 * jnp.log(x)**2,\n",
        "      name='signal_variance')\n",
        "  length_scale = yield spm.ModelParameter.from_prior(\n",
        "    tfd.Sample(\n",
        "        tfd.LogNormal(loc=0., scale=1.),\n",
        "        sample_shape=[data_dimensionality],\n",
        "        name='length_scale'),\n",
        "    constraint=spm.Constraint(bounds=(0.0, None)))\n",
        "  kernel = tfpk.MaternFiveHalves(\n",
        "      amplitude=jnp.sqrt(signal_variance), validate_args=True)\n",
        "  kernel = tfpk.FeatureScaled(\n",
        "      kernel, scale_diag=length_scale, validate_args=True)\n",
        "  observation_noise_variance = yield spm.ModelParameter(\n",
        "      init_fn=lambda x: jnp.exp(random.normal(x)),\n",
        "      constraint=spm.Constraint(bounds=(0.0, 100.0), bijector=tfb.Softplus()),\n",
        "      regularizer=lambda x: 0.01 * jnp.log(x)**2,\n",
        "      name='observation_noise_variance')\n",
        "  return tfd.GaussianProcess(\n",
        "      kernel=kernel,\n",
        "      index_points=inputs,\n",
        "      observation_noise_variance=observation_noise_variance,\n",
        "      validate_args=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTBcby0FzZjT"
      },
      "source": [
        "To build a GP Flax module, instantiate a `StochasticProcessModel` with a GP coroutine as shown below. The module runs the coroutine in the `setup` and `__call__` methods to initialize the parameters and then instantiate the GP object with the given parameters.\n",
        "\n",
        "Recall that Flax modules have two primary methods: `init`, which initializes parameters, and `apply`, which computes the model's forward pass given a set of parameters and input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BW8iIFI3j6ad"
      },
      "outputs": [],
      "source": [
        "model = spm.StochasticProcessModel(coroutine=vizier_gp_coroutine)\n",
        "\n",
        "# Sample some fake data.\n",
        "# Assume we have `num_points` observations, each with `dim` features.\n",
        "num_points = 12\n",
        "\n",
        "# Sample a set of index points.\n",
        "index_points = np.random.normal(\n",
        "    size=[num_points, data_dimensionality]).astype(np.float32)\n",
        "\n",
        "# Sample function values observed at the index points\n",
        "observations = np.random.normal(size=[num_points]).astype(np.float32)\n",
        "\n",
        "# Call the Flax module's `init` method to obtain initial parameter values.\n",
        "init_params = model.init(random.PRNGKey(0), index_points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssgl1ULJlQf6"
      },
      "source": [
        "We can observe the initial parameters values of the Flax model and see that they match with the 'ModelParameter' definitions in our coroutine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR1mQBLZltro"
      },
      "outputs": [],
      "source": [
        "print(init_params['params'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61BuqfvblrfI"
      },
      "source": [
        "To instantiate a GP with a set of parameters and index points, use the Flax module's `apply` method. `apply` also returns the regularization `losses` for the parameters, in `mutables`. The regularization `losses` are treated as mutable state because they are recomputed internally with each forward pass of the model. For more on mutable state in Flax, see [this](https://flax.readthedocs.io/en/latest/guides/state_params.html) tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Uold94l_yJ"
      },
      "outputs": [],
      "source": [
        "gp, mutables = model.apply(\n",
        "    init_params,\n",
        "    index_points,\n",
        "    mutable=['losses'])\n",
        "assert isinstance(gp, tfd.GaussianProcess)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBRCCmBzvkRg"
      },
      "source": [
        "## Optimizing hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMxnVHOyIxHI"
      },
      "source": [
        "### Exercise: Loss function\n",
        "Write down a loss function that takes a parameters dict and returns the loss value, using `model.apply`. The function will close over the observed data.\n",
        "\n",
        "The loss should be the sum of the GP negative log likelihood and the regularization losses. The regularization loss values are computed when the module is called, using the `ModelParameter` regularization functions. They are stored in a mutable variable collection called `\"losses\"`, using the Flax method [`sow`](https://flax.readthedocs.io/en/latest/api_reference/flax.linen.html?highlight=sow#flax.linen.Module.sow)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYOpS460J4SC"
      },
      "outputs": [],
      "source": [
        "def loss_fn(params):\n",
        "  ...\n",
        "  return loss, {}  # Return an empty dict as auxiliary state."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-7KMSm7OfWR"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Kx1AbzwvoA3"
      },
      "outputs": [],
      "source": [
        "def loss_fn(params):\n",
        "  gp, mutables = model.apply({'params': params},\n",
        "                              index_points,\n",
        "                              mutable=['losses'])\n",
        "  loss = (-gp.log_prob(observations) +\n",
        "          jax.tree_util.tree_reduce(jnp.add, mutables['losses']))  # add the regularization losses.\n",
        "  return loss, {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiVS3CZoAWoF"
      },
      "source": [
        "The gradients of the loss have the same structure as the `params` dict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X31_PEZ_3XM"
      },
      "outputs": [],
      "source": [
        "grads = jax.grad(loss_fn, has_aux=True)(init_params['params'])[0]\n",
        "print(grads)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z1WCXl0DUZg"
      },
      "source": [
        "We can use `jax.tree_util` to take a step along the gradient (though in practice, with Optax, we can use `update` and `apply_updates` to update the parameters at each train step)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz66WUMmDkI3"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "updated_params = jax.tree_util.tree_map(\n",
        "    lambda p, g: p - learning_rate * g,\n",
        "    init_params['params'],\n",
        "    grads)\n",
        "print(updated_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fN3hdzETKARf"
      },
      "source": [
        "## Optimize hyperparameters with Vizier optimizers\n",
        "\n",
        "Flax modules are often optimized using Optax which requires the developer to write a routine that initializes parameter values and then repeatedly computes the loss function gradients and updates the parameter values accordingly.\n",
        "\n",
        "[Vizier Optimizers](TODO) is a library of optimizers that automate the process of finding the optimal Flax parameter values and wrap optimizers from libraries such as Optax and Jaxopt in a common interface. To use a Vizier Optimizer you have to specify the following:\n",
        "- `setup` function which is used to generate the initial parameter values.\n",
        "- `loss_fn` function which is used for computing the loss function value and gradients. For example, the loss function of a GP model would be a marginal likelihood plus the parameters regularizations.\n",
        "- `rng` PRNGKey for controlling pseudo randomization.\n",
        "- `constraints` on the parameters (optional).\n",
        "\n",
        "Below we use the Vizier `JaxoptLbfgsB` optimizer to run a constrained L-BFGS-B algorithm. Unconstrainted optimizers (e.g. Adam) use a bijector function to map between the unconstrained space where the search is performed, and the constrained space where the loss function is evaluated. On the contrary, constrained optimizers (e.g. L-BGFS-B) use the constraint bounds directly in the search process.\n",
        "\n",
        "To pass the constraints bounds to the `JaxoptLbfgsB` optimizer we use the `spm.get_constraints` function that traverse the parameters defined in the module coroutine and extract their bounds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w7_N0fPJ9p7"
      },
      "outputs": [],
      "source": [
        "setup = lambda rng: model.init(rng, index_points)['params']\n",
        "model_optimizer = optimizers.JaxoptLbfgsB(\n",
        "      random_restarts=20, best_n=None\n",
        "  )\n",
        "constraints = spm.get_constraints(model.coroutine)\n",
        "optimal_params, _ = model_optimizer(setup, loss_fn, random.PRNGKey(0),\n",
        "                                    constraints=constraints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohHdQfjtCqIq"
      },
      "source": [
        "## Predict on new inputs, conditional on observations\n",
        "\n",
        "To compute the posterior predictive GP on unseen points, conditioned on observed data, use the `precompute_predictive` and `predict` methods of the Flax module. `precompute_predictive` must be called first; it runs and stores the Cholesky decomposition of the kernel matrix for the observed data. `predict` then returns a posterior predictive GP at new index points, avoiding recomputation of the Cholesky."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNIKrHskCsRf"
      },
      "outputs": [],
      "source": [
        "# Precompute the Cholesky.\n",
        "_, pp_state = model.apply(\n",
        "    {'params': optimal_params},\n",
        "    index_points,\n",
        "    observations,\n",
        "    mutable=['predictive'],\n",
        "    method=model.precompute_predictive)\n",
        "\n",
        "# Predict on new index points.\n",
        "predictive_index_points = np.random.normal(\n",
        "    size=[5, data_dimensionality]).astype(np.float32)\n",
        "pp_dist = model.apply(\n",
        "    {'params': optimal_params, **pp_state},\n",
        "    predictive_index_points,\n",
        "    method=model.predict)\n",
        "\n",
        "# `predict` returns a TFP distribution, whose mean, variance, and samples we can\n",
        "# use to compute an acquisition function.\n",
        "assert pp_dist.mean().shape == (5,)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBrxUYVjjDaL"
      },
      "source": [
        "## Optimize a black-box function\n",
        "\n",
        "For an end-to-end example of Bayesian optimization, we'll use the GP surrogate model defined above along with an Upper Confidence Bound acquisition function to try to find the maximum of the Weierstrass function. First, visualize the function surface."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_SF1D7Vu1sp"
      },
      "outputs": [],
      "source": [
        "# Use the Weierstrass function from Vizier's Black-Box Optimization Benchmarking\n",
        "# (BBOB) library.\n",
        "bb_fun = bbob.Weierstrass\n",
        "\n",
        "# Sample a set of index points in a 2D space.\n",
        "num_points = 6\n",
        "max_x = np.array(2.).astype(np.float32)\n",
        "index_points = random.uniform(\n",
        "    random.PRNGKey(3), \n",
        "    shape=[num_points, data_dimensionality], dtype=jnp.float32) * max_x\n",
        "\n",
        "# Compute function values observed at the index points.\n",
        "observations = np.apply_along_axis(\n",
        "    bb_fun, axis=1, arr=index_points).astype(np.float32)\n",
        "\n",
        "# Define a grid of points in the function domain for plotting.\n",
        "n_grid = 100\n",
        "x = y = np.linspace(0, max_x, n_grid, dtype=np.float32)\n",
        "X, Y = np.meshgrid(x, y)\n",
        "x_grid = np.vstack([X.ravel(), Y.ravel()]).T\n",
        "y_grid = np.apply_along_axis(bb_fun, axis=1, arr=x_grid)\n",
        "Z = y_grid.reshape(X.shape)\n",
        "\n",
        "# Plot the black-box function values.\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(X, Y, Z, alpha=0.5)\n",
        "ax.scatter(index_points[:, 0], index_points[:, 1], observations, color='r', \n",
        "           label='Initial observed data')\n",
        "plt.title('Black-box (Weierstrass) function values and observed data')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZA9C41oCBm"
      },
      "source": [
        "Next, run a few iterations of Bayesian optimization to maximize the black-box function given the observed data. A single iteration consists of the following steps:\n",
        "1. Optimize the GP hyperparameters.\n",
        "2. Find a suggestion that maximizes an Upper Confidence Bound acquisition function. In this example, we use grid search for the optimization.\n",
        "3. Evaluate the black-box function on the suggestion and append it to the set of observed data.\n",
        "\n",
        "(Note that this simple Bayesopt algorithm is for educational purposes and that we'd expect Vizier's GP bandit algorithm to give better results.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VqoyPzWOLN7"
      },
      "outputs": [],
      "source": [
        "num_bayesopt_iter = 5\n",
        "\n",
        "# At each iteration, redefine the loss function given the current observed data.\n",
        "def build_loss_fn(index_points, observations):\n",
        "  def loss_fn(params):\n",
        "    gp, mutables = model.apply({'params': params},\n",
        "                                index_points,\n",
        "                                mutable=['losses'])\n",
        "    loss = (-gp.log_prob(observations) +\n",
        "            jax.tree_util.tree_reduce(jnp.add, mutables['losses']))  # add the regularization losses.\n",
        "    return loss, {}\n",
        "  return loss_fn\n",
        "\n",
        "for i in range(num_bayesopt_iter):\n",
        "  # Update the loss function to condition on all observed data.\n",
        "  loss_fn = build_loss_fn(index_points, observations)\n",
        "\n",
        "  # Optimize the GP hyperparameters.\n",
        "  optimal_params, _ = model_optimizer(setup, loss_fn, random.PRNGKey(0),\n",
        "                                      constraints=constraints)\n",
        "\n",
        "  # Compute the posterior predictive distribution over a grid of points in the\n",
        "  # function domain (x_grid).\n",
        "  _, pp_state = model.apply(\n",
        "      {'params': optimal_params},\n",
        "      index_points,\n",
        "      observations,\n",
        "      mutable=['predictive'],\n",
        "      method=model.precompute_predictive)\n",
        "  pp_dist = model.apply(\n",
        "      {'params': optimal_params, **pp_state},\n",
        "      x_grid,\n",
        "      method=model.predict)\n",
        "\n",
        "  # Compute the acquisition function value at each point in the grid.\n",
        "  pred_mean = pp_dist.mean()\n",
        "  ucb_vec = pred_mean + 2. * pp_dist.stddev()\n",
        "\n",
        "  # Find the grid point with the highest acquisition function value.\n",
        "  ind = np.argmax(ucb_vec)\n",
        "\n",
        "  # Evaluate the black box function at the selected point. \n",
        "  f_val = bb_fun(x_grid[ind])\n",
        "\n",
        "  # Visualize the surrogate model mean and acquisition function surface at this\n",
        "  # iteration.\n",
        "  fig = plt.figure(figsize=(10, 10))\n",
        "  ax = fig.add_subplot(121, projection='3d')\n",
        "  W = pred_mean.reshape(X.shape)\n",
        "  ax.plot_surface(X, Y, W, alpha=0.5)\n",
        "  ax.scatter(index_points[:, 0], index_points[:, 1], observations, color='r',\n",
        "             label='Observed data')\n",
        "  ax.set_title('Observed data and posterior predictive GP mean')\n",
        "  ax.legend()\n",
        "  \n",
        "  ax = fig.add_subplot(122, projection='3d')\n",
        "  ucb = ucb_vec.reshape(X.shape)\n",
        "  ax.plot_surface(X, Y, ucb, alpha=0.5)\n",
        "  ax.scatter(*x_grid[ind], ucb_vec[ind], color='r', label='New suggestion')\n",
        "  ax.set_title('Acquisition function')\n",
        "  ax.legend()\n",
        "  plt.show()\n",
        "\n",
        "  # Append the new suggestion and function value to the set of observations.\n",
        "  index_points = np.concatenate([index_points, x_grid[ind][np.newaxis]])\n",
        "  observations = np.concatenate(\n",
        "      [observations, np.array(f_val).astype(np.float32)[np.newaxis]])\n",
        "  \n",
        "  print(f'Iteration: {i}')\n",
        "  print(f'Acquisition function value at suggestion: {ucb_vec[ind]}')\n",
        "  print(f'Black-box function value at suggestion: {f_val}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPM-5NKX5cjM"
      },
      "source": [
        "## Deeper dive on selected topics in TFP\n",
        "As shown above, the Flax GP model makes use of a number of TFP components:\n",
        "- Distributions specify parameter priors (e.g. `tfd.Gamma`). The stochastic process model itself is also a TFP distribution, `tfd.GaussianProcess`.\n",
        "- Bijectors (e.g. `tfb.Softplus`) are used to constrain parameters for optimization, and may also be used for input/output warping.\n",
        "- PSD kernels (e.g. `tfpk.ExponentiatedQuadratic`) specify the kernel function for the stochastic process.\n",
        "\n",
        "The next sections of this Colab introduce these and how they're used in Bayesopt modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuU739_PDX6i"
      },
      "source": [
        "### `tfd.GaussianProcess` and friends\n",
        "The stochastic process Flax modules return a TFP distribution in the [Gaussian Process](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/distributions/gaussian_process.py#L93) family (an instance of `tfd.GaussianProcess`, `tfd.StudentTProcess`, or `tfde.MultiTaskGaussianProcess`).\n",
        "\n",
        "This Colab doesn't go into detail on TFP distributions, since advanced usage and implementation of distributions is rarely required for Bayesopt modeling with Vizier. For an overview of TFP distributions, see [TensorFlow Distributions: A Gentle Introduction](https://www.tensorflow.org/probability/examples/TensorFlow_Distributions_Tutorial).\n",
        "\n",
        "Some of the methods of the Gaussian Process distribution are demonstrated below. [Gaussian Process Regression in TFP](https://www.tensorflow.org/probability/examples/Gaussian_Process_Regression_In_TFP) is also worth reading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S1ULLbnpDj_2"
      },
      "outputs": [],
      "source": [
        "# Build a kernel function (see \"PSD kernels\" section below) and GP.\n",
        "num_points = 6\n",
        "index_points = random.uniform(\n",
        "    random.PRNGKey(3), \n",
        "    shape=[num_points, data_dimensionality], dtype=jnp.float32) \n",
        "observations = random.uniform(\n",
        "    random.PRNGKey(4), \n",
        "    shape=[num_points], dtype=jnp.float32) \n",
        "\n",
        "kernel = tfpk.MaternFiveHalves(\n",
        "    amplitude=2.,\n",
        "    length_scale=0.3,\n",
        "    validate_args=True  # Run additional runtime checks; possibly expensive.\n",
        "    )\n",
        "observation_noise_variance = jnp.ones([], dtype=observations.dtype)\n",
        "gp = tfd.GaussianProcess(\n",
        "    kernel,\n",
        "    index_points=index_points,\n",
        "    observation_noise_variance=observation_noise_variance,\n",
        "    always_yield_multivariate_normal=True,  # See commentary below.\n",
        "    cholesky_fn=lambda x: tfp.experimental.distributions.marginal_fns.retrying_cholesky(x)[0],  # See commentary below.\n",
        "    validate_args=True)\n",
        "\n",
        "# Take 4 samples from the GP at the index points.\n",
        "s = gp.sample(4, seed=random.PRNGKey(0))\n",
        "assert s.shape == (4, num_points)\n",
        "\n",
        "# Compute the log likelihood of the sampled values.\n",
        "lp = gp.log_prob(s)\n",
        "assert lp.shape == (4,)\n",
        "\n",
        "# GPs can also be instantiated without index points, in which case the index\n",
        "# points must be passed to method calls.\n",
        "gp_no_index_pts = tfd.GaussianProcess(\n",
        "    kernel,\n",
        "    observation_noise_variance=observation_noise_variance)\n",
        "s = gp_no_index_pts.sample(index_points=index_points, seed=random.PRNGKey(0))\n",
        "\n",
        "# Predictive GPs conditioned on observations can be built with\n",
        "# `GaussianProcess.posterior_predictive`. The Flax module's\n",
        "# `precompute_predictive` and `predict` methods call this GP method.\n",
        "gprm = gp.posterior_predictive(\n",
        "    observations=observations,\n",
        "    predictive_index_points=predictive_index_points)\n",
        "\n",
        "# `gprm` is an instance of `tfd.GaussianProcessRegressionModel`. This class can\n",
        "# also be instantiated directly (as a side note -- this isn't necessary for\n",
        "# modeling with Vizier).\n",
        "same_gprm = tfd.GaussianProcessRegressionModel(\n",
        "    kernel,\n",
        "    index_points=predictive_index_points,\n",
        "    observation_index_points=index_points,\n",
        "    observations=observations,\n",
        "    observation_noise_variance=observation_noise_variance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUwFGcRxPa2F"
      },
      "source": [
        "Aside from the kernel, index points, and noise variance, there are two constructor args of `GaussianProcess` to be aware of:\n",
        "- `always_yield_multivariate_normal`. By default, if there is only a single index point (`index_points` has shape `[1, d]`), then the Gaussian process has a univariate marginal distribution, so methods like `mean`, `stddev`, and `sample` will return a scalar. (If `index_points` has shape `[n, d]` the output of these methods will have shape `[n]`). To override the behavior for the univariate case and return arrays of shape `[1]` instead of scalars, set `always_yield_multivariate_normal=True`.\n",
        "- `cholesky_fn` is a callable that takes a matrix and returns a Cholesky-like lower triangular factor. The default function adds a jitter of 1e-6 to the diagonal and then calls `jnp.linalg.cholesky`. An alternative, used in the Vizier GP, is `tfp.experimental.distributions.marginal_fns.retrying_cholesky`, which adds progressively larger jitter until the Cholesky decomposition succeeds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QkOf892WVX3"
      },
      "source": [
        "### A side note on batch shape in TFP\n",
        "tl;dr: Don't worry about batch shape.\n",
        "\n",
        "TFP objects have a notion of batch shape, which is useful for vectorized computations. For more on this, see [Understanding TensorFlow Distributions Shapes](https://www.tensorflow.org/probability/examples/Understanding_TensorFlow_Distributions_Shapes).\n",
        "\n",
        "For the purposes of Bayesopt in Vizier, JAX's `vmap` means that our TFP objects can have a single parameterization with empty batch shape. For example, in the following loss function takes a scalar `amplitude`, and the kernel and GP both have empty batch shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIhhwTvYWmVG"
      },
      "outputs": [],
      "source": [
        "def loss_fn(amplitude):  # `a` is a scalar.\n",
        "  k = tfpk.ExponentiatedQuadratic(amplitude=amplitude)  # batch shape []\n",
        "  gp = tfd.GaussianProcess(k, index_points=index_points)   # batch shape []\n",
        "  return -gp.log_prob(observations)\n",
        "\n",
        "initial_amplitude = np.random.uniform(size=[50])\n",
        "\n",
        "losses = jax.vmap(loss_fn)(initial_amplitude)\n",
        "assert losses.shape == (50,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQyX3m2I4QSv"
      },
      "source": [
        "We could also vectorize the loss computation by using a batched GP. In this simple case, the code is identical except that `vmap` is removed. Now, the kernel and GP represent a \"batch\" of kernels and GPs, each with different parameter values. Working with batch shape requires additional accounting on the part of the user to ensure that parameter shapes broadcast correctly, the correct dimensions are reduced over, etc. For Vizier's use case, we find it simpler to rely on `vmap`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFWBcAw84daN"
      },
      "outputs": [],
      "source": [
        "def loss_fn(amplitude):  # `a` has shape [50].\n",
        "  k = tfpk.ExponentiatedQuadratic(amplitude=amplitude)  # batch shape [50]\n",
        "  gp = tfd.GaussianProcess(k, index_points=index_points)   # batch shape [50]\n",
        "  return -gp.log_prob(observations)\n",
        "\n",
        "initial_amplitude = np.random.uniform(size=[50])\n",
        "\n",
        "# No vmap.\n",
        "losses = loss_fn(initial_amplitude)\n",
        "assert losses.shape == (50,)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "1bhe9vVJps8t8IsIU4sbInYvcQPgBlLWn",
          "timestamp": 1667943943389
        }
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
